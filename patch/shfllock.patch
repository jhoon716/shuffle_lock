diff -uNr a/kernel/locking/mcs_spinlock.h b/kernel/locking/mcs_spinlock.h
--- a/kernel/locking/mcs_spinlock.h	2023-10-26 18:44:12.610209465 +0000
+++ b/kernel/locking/mcs_spinlock.h	2023-10-26 18:44:26.150176226 +0000
@@ -17,8 +17,21 @@
 
 struct mcs_spinlock {
 	struct mcs_spinlock *next;
-	int locked; /* 1 if lock acquired */
-	int count;  /* nesting count, see qspinlock.c */
+	union
+	{
+		int locked; /* 1 if lock acquired */
+		struct
+		{
+			u8 lstatus;
+			u8 sleader;
+			u16 wcount;
+		};
+	};
+	int count; /* nesting count, see qspinlock.c */
+
+	int nid;
+	int cid;
+	struct mcs_spinlock *last_visited;
 };
 
 #ifndef arch_mcs_spin_lock_contended
diff -uNr a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
--- a/kernel/locking/qspinlock.c	2023-10-26 18:44:08.522220141 +0000
+++ b/kernel/locking/qspinlock.c	2023-10-26 18:44:26.150176226 +0000
@@ -67,7 +67,7 @@
  */
 
 #include "mcs_spinlock.h"
-#define MAX_NODES	4
+#define MAX_NODES 4
 
 /*
  * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in
@@ -95,7 +95,7 @@
  * progress.
  */
 #ifndef _Q_PENDING_LOOPS
-#define _Q_PENDING_LOOPS	1
+#define _Q_PENDING_LOOPS 1
 #endif
 
 /*
@@ -108,6 +108,287 @@
  */
 static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);
 
+/* Per-CPU pseudo-random number seed */
+static DEFINE_PER_CPU(u32, seed);
+
+static inline void set_sleader(struct mcs_spinlock *node,
+			       struct mcs_spinlock *qend)
+{
+	smp_store_release(&node->sleader, 1);
+	if (qend != node)
+		smp_store_release(&node->last_visited, qend);
+}
+
+static inline void clear_sleader(struct mcs_spinlock *node)
+{
+	node->sleader = 0;
+}
+
+static inline void set_waitcount(struct mcs_spinlock *node, int count)
+{
+	smp_store_release(&node->wcount, count);
+}
+
+#define AQS_MAX_LOCK_COUNT 256
+#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)
+
+/*
+ * xorshift function for generating pseudo-random numbers:
+ * https://en.wikipedia.org/wiki/Xorshift
+ */
+static inline u32 xor_random(void)
+{
+	u32 v;
+
+	v = this_cpu_read(seed);
+	if (v == 0)
+		get_random_bytes(&v, sizeof(u32));
+
+	v ^= v << 6;
+	v ^= v >> 21;
+	v ^= v << 7;
+	this_cpu_write(seed, v);
+
+	return v;
+}
+
+/*
+ * Return false with probability 1 / @range.
+ * @range must be a power of 2.
+ */
+#define INTRA_SOCKET_HANDOFF_PROB_ARG 0x10000
+
+static bool probably(void)
+{
+	u32 v;
+	return xor_random() & (INTRA_SOCKET_HANDOFF_PROB_ARG - 1);
+	v = this_cpu_read(seed);
+	if (v >= 2048) {
+		this_cpu_write(seed, 0);
+		return false;
+	}
+	this_cpu_inc(seed);
+	return true;
+}
+
+/*
+ * This function is responsible for aggregating waiters in a
+ * particular socket in one place up to a certain batch count.
+ * The invariant is that the shuffle leaders always start from
+ * the very next waiter and they are selected ahead in the queue,
+ * if required. Moreover, none of the waiters will be behind the
+ * shuffle leader, they are always ahead in the queue.
+ * Currently, only one shuffle leader is chosen.
+ * TODO: Another aggressive approach could be to use HOH locking
+ * for n shuffle leaders, in which n corresponds to the number
+ * of sockets.
+ */
+static void shuffle_waiters(struct qspinlock *lock, struct mcs_spinlock *node,
+			    int is_next_waiter)
+{
+	struct mcs_spinlock *curr, *prev, *next, *last, *sleader, *qend;
+	int nid;
+	int curr_locked_count;
+	int one_shuffle = false;
+
+	prev = smp_load_acquire(&node->last_visited);
+	if (!prev)
+		prev = node;
+	last = node;
+	curr = NULL;
+	next = NULL;
+	sleader = NULL;
+	qend = NULL;
+
+	nid = node->nid;
+	curr_locked_count = node->wcount;
+
+	barrier();
+
+	/*
+	 * If the wait count is 0, then increase node->wcount
+	 * to 1 to avoid coming it again.
+	 */
+	if (curr_locked_count == 0) {
+		set_waitcount(node, ++curr_locked_count);
+	}
+
+	/*
+	 * Our constraint is that we will reset every shuffle
+	 * leader and the new one will be selected at the end,
+	 * if any.
+	 *
+	 * This one here is to avoid the confusion of having
+	 * multiple shuffling leaders.
+	 */
+	clear_sleader(node);
+
+	/*
+	 * In case the curr_locked_count has crossed a
+	 * threshold, which is certainly impossible in this
+	 * design, then load the very next of the node and pass
+	 * the shuffling responsibility to that @next.
+	 */
+	/* if (curr_locked_count >= AQS_MAX_LOCK_COUNT) { */
+	if (!probably()) {
+		sleader = READ_ONCE(node->next);
+		goto out;
+	}
+
+	/*
+	 * In this loop, we try to shuffle the wait queue at
+	 * least once to allow waiters from the same socket to
+	 * have no cache-line bouncing. This shuffling is
+	 * associated in two aspects:
+	 * 1) when both adjacent nodes belong to the same socket
+	 * 2) when there is an actual shuffling that happens.
+	 *
+	 * Currently, the approach is very conservative. If we
+	 * miss any of the elements while traversing, we return
+	 * back.
+	 *
+	 * TODO: We can come up with some aggressive strategy to
+	 * form long chains, which we are yet to explore
+	 *
+	 * The way the algorithm works is that it tries to have
+	 * at least two pointers: pred and curr, in which
+	 * curr = pred->next. If curr and pred are in the same
+	 * socket, then no need to shuffle, just update pred to
+	 * point to curr.
+	 * If that is not the case, then try to find the curr
+	 * whose node id is same as the @node's node id. On
+	 * finding that, we also try to get the @next, which is
+	 * next = curr->next; which we use all of them to
+	 * shuffle them wrt @last.
+	 * @last holds the latest shuffled element in the wait
+	 * queue, which is updated on each shuffle and is most
+	 * likely going to be next shuffle leader.
+	 */
+	for (;;) {
+		/*
+		 * Get the curr first
+		 */
+		curr = READ_ONCE(prev->next);
+
+		/*
+		 * Now, right away we can quit the loop if curr
+		 * is NULL or is at the end of the wait queue
+		 * and choose @last as the sleader.
+		 */
+		if (!curr) {
+			sleader = last;
+			qend = prev;
+			break;
+		}
+
+recheck_curr_tail:
+		/*
+		 * If we are the last one in the tail, then
+		 * we cannot do anything, we should return back
+		 * while selecting the next sleader as the last one
+		 */
+		if (curr->cid ==
+		    (atomic_read(&lock->val) >> _Q_TAIL_CPU_OFFSET)) {
+			sleader = last;
+			qend = prev;
+			break;
+		}
+
+		/* got the current for sure */
+
+		/* Check if curr->nid is same as nid */
+		if (curr->nid == nid) {
+			/*
+			 * if prev->nid == curr->nid, then
+			 * just update the last and prev
+			 * and proceed forward
+			 */
+			if (prev->nid == nid) {
+				set_waitcount(curr, curr_locked_count);
+
+				last = curr;
+				prev = curr;
+				one_shuffle = true;
+			} else {
+				/* prev->nid is not same, then we need
+				 * to find next and move @curr to
+				 * last->next, while linking @prev->next
+				 * to next.
+				 *
+				 * NOTE: We do not update @prev here
+				 * because @curr has been already moved
+				 * out.
+				 */
+				next = READ_ONCE(curr->next);
+				if (!next) {
+					sleader = last;
+					qend = prev;
+					/* qend = curr; */
+					break;
+				}
+
+				/*
+				 * Since, we have curr and next,
+				 * we mark the curr that it has been
+				 * shuffled and shuffle the queue
+				 */
+				set_waitcount(curr, curr_locked_count);
+
+				/*
+				 *                                                 (1)
+				 *                                    (3)       ----------
+				 *                          -------------------|--\      |
+				 *                        /                    |   v     v
+				 *   ----          ----   |  ----        ----/   ----   ----
+				 *  | SL | -> ... |Last| -> | X  |....->|Prev|->|Curr|->|Next|->....
+				 *   ----          ----  ->  ----        ----    ----  | ----
+				 *                      /          (2)                /
+				 *                      -----------------------------
+				 *                              |
+				 *                              V
+				 *   ----          ----      ----      ----        ----    ----
+				 *  | SL | -> ... |Last| -> |Curr| -> | X  |....->|Prev|->|Next|->....
+				 *   ----          ----      ----      ----        ----    ----
+				 *
+				 */
+				prev->next = next;
+				curr->next = last->next;
+				last->next = curr;
+				smp_wmb();
+
+				last = curr;
+				curr = next;
+				one_shuffle = true;
+
+				goto recheck_curr_tail;
+			}
+		} else
+			prev = curr;
+
+		/*
+		 * Currently, we only exit once we have at least
+		 * one shuffler if the shuffling leader is the
+		 * very next lock waiter.
+		 * TODO: This approach can be further optimized.
+		 */
+		if (one_shuffle) {
+			if ((is_next_waiter &&
+			     !(atomic_read_acquire(&lock->val) &
+			       _Q_LOCKED_PENDING_MASK)) ||
+			    (!is_next_waiter && READ_ONCE(node->lstatus))) {
+				sleader = last;
+				qend = prev;
+				break;
+			}
+		}
+	}
+
+out:
+	if (sleader) {
+		set_sleader(sleader, qend);
+	}
+}
+
 /*
  * We must be able to distinguish between no-tail and the tail at 0:0,
  * therefore increment the cpu number by one.
@@ -117,7 +398,7 @@
 {
 	u32 tail;
 
-	tail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;
+	tail = (cpu + 1) << _Q_TAIL_CPU_OFFSET;
 	tail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */
 
 	return tail;
@@ -126,19 +407,17 @@
 static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 {
 	int cpu = (tail >> _Q_TAIL_CPU_OFFSET) - 1;
-	int idx = (tail &  _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
+	int idx = (tail & _Q_TAIL_IDX_MASK) >> _Q_TAIL_IDX_OFFSET;
 
 	return per_cpu_ptr(&qnodes[idx].mcs, cpu);
 }
 
-static inline __pure
-struct mcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)
+static inline __pure struct mcs_spinlock *
+grab_mcs_node(struct mcs_spinlock *base, int idx)
 {
 	return &((struct qnode *)base + idx)->mcs;
 }
 
-#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)
-
 #if _Q_PENDING_BITS == 8
 /**
  * clear_pending - clear the pending bit.
@@ -180,8 +459,8 @@
 	 * We can use relaxed semantics since the caller ensures that the
 	 * MCS node is properly initialized before updating the tail.
 	 */
-	return (u32)xchg_relaxed(&lock->tail,
-				 tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;
+	return (u32)xchg_relaxed(&lock->tail, tail >> _Q_TAIL_OFFSET)
+	       << _Q_TAIL_OFFSET;
 }
 
 #else /* _Q_PENDING_BITS == 8 */
@@ -247,7 +526,8 @@
  * *,*,* -> *,1,*
  */
 #ifndef queued_fetch_set_pending_acquire
-static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)
+static __always_inline u32
+queued_fetch_set_pending_acquire(struct qspinlock *lock)
 {
 	return atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);
 }
@@ -264,30 +544,37 @@
 	WRITE_ONCE(lock->locked, _Q_LOCKED_VAL);
 }
 
-
 /*
  * Generate the native code for queued_spin_unlock_slowpath(); provide NOPs for
  * all the PV callbacks.
  */
 
-static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }
+static __always_inline void __pv_init_node(struct mcs_spinlock *node)
+{
+}
 static __always_inline void __pv_wait_node(struct mcs_spinlock *node,
-					   struct mcs_spinlock *prev) { }
+					   struct mcs_spinlock *prev)
+{
+}
 static __always_inline void __pv_kick_node(struct qspinlock *lock,
-					   struct mcs_spinlock *node) { }
-static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,
-						   struct mcs_spinlock *node)
-						   { return 0; }
-
-#define pv_enabled()		false
-
-#define pv_init_node		__pv_init_node
-#define pv_wait_node		__pv_wait_node
-#define pv_kick_node		__pv_kick_node
-#define pv_wait_head_or_lock	__pv_wait_head_or_lock
+					   struct mcs_spinlock *node)
+{
+}
+static __always_inline u32 __pv_wait_head_or_lock(struct qspinlock *lock,
+						  struct mcs_spinlock *node)
+{
+	return 0;
+}
+
+#define pv_enabled() false
+
+#define pv_init_node __pv_init_node
+#define pv_wait_node __pv_wait_node
+#define pv_kick_node __pv_kick_node
+#define pv_wait_head_or_lock __pv_wait_head_or_lock
 
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
-#define queued_spin_lock_slowpath	native_queued_spin_lock_slowpath
+#define queued_spin_lock_slowpath native_queued_spin_lock_slowpath
 #endif
 
 #endif /* _GEN_PV_LOCK_SLOWPATH */
@@ -313,19 +600,21 @@
  * contended             :    (*,x,y) +--> (*,0,0) ---> (*,0,1) -'  :
  *   queue               :         ^--'                             :
  */
-void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
+void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock,
+						  u32 val)
 {
 	struct mcs_spinlock *prev, *next, *node;
 	u32 old, tail;
 	int idx;
+	int cid;
 
 	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
 
 	if (pv_enabled())
 		goto pv_queue;
 
-	if (virt_spin_lock(lock))
-		return;
+	// if (virt_spin_lock(lock))
+	// 	return;
 
 	/*
 	 * Wait for in-progress pending->locked hand-overs with a bounded
@@ -335,8 +624,8 @@
 	 */
 	if (val == _Q_PENDING_VAL) {
 		int cnt = _Q_PENDING_LOOPS;
-		val = atomic_cond_read_relaxed(&lock->val,
-					       (VAL != _Q_PENDING_VAL) || !cnt--);
+		val = atomic_cond_read_relaxed(
+			&lock->val, (VAL != _Q_PENDING_VAL) || !cnt--);
 	}
 
 	/*
@@ -360,7 +649,6 @@
 	 * on @next to become !NULL.
 	 */
 	if (unlikely(val & ~_Q_LOCKED_MASK)) {
-
 		/* Undo PENDING if we set it. */
 		if (!(val & _Q_PENDING_MASK))
 			clear_pending(lock);
@@ -388,7 +676,7 @@
 	 * 0,1,0 -> 0,0,1
 	 */
 	clear_pending_set_locked(lock);
-	lockevent_inc(lock_pending);
+	// lockevent_inc(lock_pending);
 	return;
 
 	/*
@@ -396,13 +684,14 @@
 	 * queuing.
 	 */
 queue:
-	lockevent_inc(lock_slowpath);
+	// lockevent_inc(lock_slowpath);
 pv_queue:
 	node = this_cpu_ptr(&qnodes[0].mcs);
 	idx = node->count++;
-	tail = encode_tail(smp_processor_id(), idx);
+	cid = smp_processor_id();
+	tail = encode_tail(cid, idx);
 
-	trace_contention_begin(lock, LCB_F_SPIN);
+	// trace_contention_begin(lock, LCB_F_SPIN);
 
 	/*
 	 * 4 nodes are allocated based on the assumption that there will
@@ -414,7 +703,7 @@
 	 * simple enough.
 	 */
 	if (unlikely(idx >= MAX_NODES)) {
-		lockevent_inc(lock_no_node);
+		// lockevent_inc(lock_no_node);
 		while (!queued_spin_trylock(lock))
 			cpu_relax();
 		goto release;
@@ -425,7 +714,7 @@
 	/*
 	 * Keep counts of non-zero index values:
 	 */
-	lockevent_cond_inc(lock_use_node2 + idx - 1, idx);
+	// lockevent_cond_inc(lock_use_node2 + idx - 1, idx);
 
 	/*
 	 * Ensure that we increment the head node->count before initialising
@@ -434,6 +723,9 @@
 	 */
 	barrier();
 
+	node->cid = cid + 1;
+	node->nid = numa_node_id();
+	node->last_visited = NULL;
 	node->locked = 0;
 	node->next = NULL;
 	pv_init_node(node);
@@ -474,7 +766,18 @@
 		WRITE_ONCE(prev->next, node);
 
 		pv_wait_node(node, prev);
-		arch_mcs_spin_lock_contended(&node->locked);
+		// arch_mcs_spin_lock_contended(&node->locked);
+		for (;;) {
+			int __val = READ_ONCE(node->lstatus);
+			if (__val)
+				break;
+
+			if (READ_ONCE(node->sleader))
+				shuffle_waiters(lock, node, false);
+
+			cpu_relax();
+		}
+		smp_acquire__after_ctrl_dep();
 
 		/*
 		 * While waiting for the MCS lock, the next pointer may have
@@ -482,9 +785,9 @@
 		 * the next pointer & prefetch the cacheline for writing
 		 * to reduce latency in the upcoming MCS unlock operation.
 		 */
-		next = READ_ONCE(node->next);
-		if (next)
-			prefetchw(next);
+		// next = READ_ONCE(node->next);
+		// if (next)
+		// 	prefetchw(next);
 	}
 
 	/*
@@ -508,12 +811,25 @@
 	 * If PV isn't active, 0 will be returned instead.
 	 *
 	 */
-	if ((val = pv_wait_head_or_lock(lock, node)))
-		goto locked;
+	// if ((val = pv_wait_head_or_lock(lock, node)))
+	// 	goto locked;
 
-	val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));
+	// val = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));
+	for (;;) {
+		int wcount;
+
+		val = atomic_read(&lock->val);
+		if (!(val & _Q_LOCKED_PENDING_MASK))
+			break;
+
+		wcount = READ_ONCE(node->wcount);
+		if (!wcount || (wcount && node->sleader))
+			shuffle_waiters(lock, node, true);
+		cpu_relax();
+	}
+	smp_acquire__after_ctrl_dep();
 
-locked:
+	// locked:
 	/*
 	 * claim the lock:
 	 *
@@ -550,14 +866,16 @@
 	/*
 	 * contended path; wait for next if not observed yet, release.
 	 */
+	next = READ_ONCE(node->next);
 	if (!next)
 		next = smp_cond_load_relaxed(&node->next, (VAL));
 
-	arch_mcs_spin_unlock_contended(&next->locked);
+	// arch_mcs_spin_unlock_contended(&next->locked);
+	smp_store_release(&next->lstatus, 1);
 	pv_kick_node(lock, next);
 
 release:
-	trace_contention_end(lock, 0);
+	// trace_contention_end(lock, 0);
 
 	/*
 	 * release the node
@@ -572,16 +890,16 @@
 #if !defined(_GEN_PV_LOCK_SLOWPATH) && defined(CONFIG_PARAVIRT_SPINLOCKS)
 #define _GEN_PV_LOCK_SLOWPATH
 
-#undef  pv_enabled
-#define pv_enabled()	true
+#undef pv_enabled
+#define pv_enabled() true
 
 #undef pv_init_node
 #undef pv_wait_node
 #undef pv_kick_node
 #undef pv_wait_head_or_lock
 
-#undef  queued_spin_lock_slowpath
-#define queued_spin_lock_slowpath	__pv_queued_spin_lock_slowpath
+#undef queued_spin_lock_slowpath
+#define queued_spin_lock_slowpath __pv_queued_spin_lock_slowpath
 
 #include "qspinlock_paravirt.h"
 #include "qspinlock.c"
